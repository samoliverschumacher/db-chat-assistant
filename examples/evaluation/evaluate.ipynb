{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '1',\n",
      "  'note': 'chooses the correct table.',\n",
      "  'response': 'We have made a total of $75.24 in Berlin.',\n",
      "  'tables': 'invoices',\n",
      "  'user_query': 'How much money have we made in Berlin?'},\n",
      " {'id': '2',\n",
      "  'note': 'deals with NULL, verifies with user that BillingState is not unique '\n",
      "          'to the entire table',\n",
      "  'response': 'The state that made the most money is California (CA).',\n",
      "  'tables': 'invoices',\n",
      "  'user_query': 'Which state made the most money?'}]\n",
      "[{'id': '3',\n",
      "  'note': 'semantic meaning of \"Type of song\" is used to select genre table',\n",
      "  'response': 'The type of music that has the longest song is \"Occupation / '\n",
      "              'Precipice\".',\n",
      "  'tables': 'tracks,genres',\n",
      "  'user_query': 'Which type of music has the longest song?'},\n",
      " {'id': '4',\n",
      "  'note': 'longer chain of related entities',\n",
      "  'response': 'The genre of music that has the longest song is \"TV Shows\".',\n",
      "  'tables': 'employees,customers,invoices,invoice_items',\n",
      "  'user_query': 'Which was the most expensive item our top employee sold?'}]\n",
      "[{'id': '5',\n",
      "  'note': 'easier test than test=3',\n",
      "  'response': 'Agent stopped due to iteration limit or time limit.',\n",
      "  'tables': 'tracks,genres',\n",
      "  'user_query': 'Which genre of music has the longest song?'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from dbchat.evaluation.utils import load_evaluation_csv_data\n",
    "from dbchat import ROOT_DIR\n",
    "fpath = ROOT_DIR.parent.parent / \"examples/evaluation/queries.csv\"\n",
    "\n",
    "eval_data = load_evaluation_csv_data( fpath, stream=True, chunksize = 2)\n",
    "for d in eval_data:\n",
    "    pprint.pprint(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approach: sql_engine_w_reranking\n",
      "database:\n",
      "  metadata:\n",
      "    document_id_like: '%-2'\n",
      "    metadata_path: sqlite:///data/chinook.db\n",
      "    table_name: table_descriptions\n",
      "  path: sqlite:///data/chinook.db\n",
      "index:\n",
      "  class: ollama\n",
      "  name: llama2reranker\n",
      "  reranking:\n",
      "    config_object: ReRankerLLMConfig\n",
      "    reranker_kwargs:\n",
      "      top_n: 3\n",
      "  retriever_kwargs:\n",
      "    similarity_top_k: 4\n",
      "llm:\n",
      "  class: ollama\n",
      "  name: llama2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "config_path = ROOT_DIR.parent / \"tests/data/inputs/cfg_3.yml\"\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "print(yaml.dump(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "Debugging Query: SELECT TABLE_NAME, DESCRIPTION FROM table_descriptions WHERE TABLE_NAME IN ('albums','artists','customers','employees','genres','invoice_items','invoices','media_types','playlist_track','playlists','artists'0,'artists'1) AND DOCUMENT_ID LIKE '%-2'\n",
      "INFO:llama_index.indices.struct_store.sql_retriever:> Table desc str: Table 'tracks' has columns: TrackId (INTEGER), Name (NVARCHAR(200)), AlbumId (INTEGER), MediaTypeId (INTEGER), GenreId (INTEGER), Composer (NVARCHAR(220)), Milliseconds (INTEGER), Bytes (INTEGER), UnitPrice (NUMERIC(10, 2)), and foreign keys: ['MediaTypeId'] -> media_types.['MediaTypeId'], ['GenreId'] -> genres.['GenreId'], ['AlbumId'] -> albums.['AlbumId']. The table description is: Tracks on a album, and details like price. Does not store a reference to playlist.\n",
      "\n",
      "Table 'invoice_items' has columns: InvoiceLineId (INTEGER), InvoiceId (INTEGER), TrackId (INTEGER), UnitPrice (NUMERIC(10, 2)), Quantity (INTEGER), and foreign keys: ['TrackId'] -> tracks.['TrackId'], ['InvoiceId'] -> invoices.['InvoiceId']. The table description is: A record of the items on invoices, including the track those items occured on the album.\n",
      "\n",
      "Table 'playlist_track' has columns: PlaylistId (INTEGER), TrackId (INTEGER), and foreign keys: ['TrackId'] -> tracks.['TrackId'], ['PlaylistId'] -> playlists.['PlaylistId']. The table description is: Maps track ids to playlist ids.\n",
      "input_query='How much money have we made in Berlin?'\n",
      "response.response='Sorry, but I\\'m a large language model, I cannot provide a response to the query \"How much money have we made in Berlin?\" as the provided SQL statement is invalid. The error message indicates that there is a problem with the syntax of the SQL statement.\\n\\nTo generate a response to this query, I would need a valid and complete SQL statement that can be executed to retrieve the desired data from the database. Can you please provide me with the correct SQL statement or more information about the query so I can help you?'\n",
      "SELECT SUM(invoice_items.UnitPrice * invoice_items.Quantity) AS total_revenue FROM invoice_items JOIN playlist_track ON invoice_items.TrackId = playlist_track.TrackId JOIN tracks ON playlist_track.PlaylistId = tracks.PlaylistId WHERE tracks.City = 'Berlin';\n",
      "retrieved_tables=[SQLTableSchema(table_name='tracks', context_str='Tracks on a album, and details like price. Does not store a reference to playlist.'), SQLTableSchema(table_name='invoice_items', context_str='A record of the items on invoices, including the track those items occured on the album.'), SQLTableSchema(table_name='playlist_track', context_str='Maps track ids to playlist ids.')]\n"
     ]
    }
   ],
   "source": [
    "from dbchat.sql_agent import create_agent\n",
    "\n",
    "query_engine = create_agent( config )\n",
    "\n",
    "eval_data = load_evaluation_csv_data( fpath, stream=False)\n",
    "input_query = eval_data[0]['user_query']\n",
    "response = query_engine.query(input_query)\n",
    "print(f\"{input_query=}\"\n",
    "        \"\\n\"\n",
    "        f\"{response.response=}\"\n",
    "        \"\\n\"\n",
    "        f\"{response.metadata['sql_query']}\")\n",
    "retrieved_tables = query_engine.sql_retriever._get_tables(input_query)\n",
    "print(f\"{retrieved_tables=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging Query: SELECT TABLE_NAME, DESCRIPTION FROM table_descriptions WHERE TABLE_NAME IN ('albums','artists','customers','employees','genres','invoice_items','invoices','media_types','playlist_track','playlists','artists'0,'artists'1) AND DOCUMENT_ID LIKE '%-2'\n",
      "[{'test_name': 'evaluate_synthetic_judge', 'config': {'approach': 'sql_engine_w_reranking', 'database': {'path': 'sqlite:///data/chinook.db', 'metadata': {'metadata_path': 'sqlite:///data/chinook.db', 'table_name': 'table_descriptions', 'document_id_like': '%-2'}}, 'index': {'name': 'llama2reranker', 'class': 'ollama', 'retriever_kwargs': {'similarity_top_k': 4}, 'reranking': {'config_object': 'ReRankerLLMConfig', 'reranker_kwargs': {'top_n': 3}}}, 'llm': {'name': 'llama2', 'class': 'ollama'}}, 'input_query': 'How much money have we made in Berlin?', 'expected_response': 'We have made a total of $75.24 in Berlin.', 'actual_response': {'response': None, 'tables': ['tracks', 'invoice_items', 'playlist_track']}, 'synthesized_judgement': CompletionResponse(text='Based on the provided information, I would rate the summarization as a 3 out of 10. The summary does not accurately reflect the content of the reference text. Here are some major differences between the two:\\n\\n* The summary completely omits any mention of the amount of money made in Berlin, which is a key detail mentioned in the reference text.\\n* The summary includes three tables, while the reference text only mentions one total amount for the trip.\\n* The summary uses curly braces `{}` to enclose the table headers, while the reference text does not use any formatting or structure to present the information.\\n\\nHere is a bullet point list of the major differences between the reference and summary:\\n\\n* Amount of money made in Berlin (missing in summary)\\n* Number of tables presented (3 in summary, 1 in reference)\\n* Formatting and structure of the table (reference text is unformatted, summary uses curly braces `{}`)', additional_kwargs={}, raw=None, delta=None)}, {'test_name': 'evaluate_synthetic_judge', 'config': {'approach': 'sql_engine_w_reranking', 'database': {'path': 'sqlite:///data/chinook.db', 'metadata': {'metadata_path': 'sqlite:///data/chinook.db', 'table_name': 'table_descriptions', 'document_id_like': '%-2'}}, 'index': {'name': 'llama2reranker', 'class': 'ollama', 'retriever_kwargs': {'similarity_top_k': 4}, 'reranking': {'config_object': 'ReRankerLLMConfig', 'reranker_kwargs': {'top_n': 3}}}, 'llm': {'name': 'llama2', 'class': 'ollama'}}, 'input_query': 'Which state made the most money?', 'expected_response': 'The state that made the most money is California (CA).', 'actual_response': {'response': None, 'tables': ['invoice_items', 'tracks', 'genres']}, 'synthesized_judgement': CompletionResponse(text='Based on the provided reference text and summary, I would give the summary a score of 2 out of 10 for its accuracy. Here\\'s why:\\n\\n* The summary does not contain any information about the state that made the most money, which is a crucial aspect of the original text.\\n* The summary contains completely unrelated keywords such as \"response\", \"tables\", and \"genres\" that have no connection to the topic of the original text.\\n\\nMajor differences between the reference and the summary are:\\n\\n* The original text mentions California (CA) as the state that made the most money, while the summary does not include any information about a state making money.\\n* The original text contains no tables or genres, while the summary includes two irrelevant keywords related to tables and genres.\\n* The original text does not mention \"response\", while the summary includes this keyword with no context or explanation.', additional_kwargs={}, raw=None, delta=None)}, {'test_name': 'evaluate_synthetic_judge', 'config': {'approach': 'sql_engine_w_reranking', 'database': {'path': 'sqlite:///data/chinook.db', 'metadata': {'metadata_path': 'sqlite:///data/chinook.db', 'table_name': 'table_descriptions', 'document_id_like': '%-2'}}, 'index': {'name': 'llama2reranker', 'class': 'ollama', 'retriever_kwargs': {'similarity_top_k': 4}, 'reranking': {'config_object': 'ReRankerLLMConfig', 'reranker_kwargs': {'top_n': 3}}}, 'llm': {'name': 'llama2', 'class': 'ollama'}}, 'input_query': 'Which type of music has the longest song?', 'expected_response': 'The type of music that has the longest song is \"Occupation / Precipice\".', 'actual_response': {'response': None, 'tables': []}, 'synthesized_judgement': CompletionResponse(text='Based on the provided information, I would give a score of 0 for the summarization. Here\\'s why:\\n\\n* The reference text clearly states that the type of music with the longest song is \"Occupation / Precipice\". However, the summary does not mention this information at all.\\n* The summary only provides a single line of code `{\\'response\\': None, \\'tables\\': []}`. This suggests that the summarization did not capture any relevant information from the reference text.\\n\\nHere are the major differences between the reference and the summary:\\n\\n* The reference text provides specific information about the type of music with the longest song, while the summary does not provide any such information.\\n* The reference text contains a complete sentence, while the summary consists of a single line of code.', additional_kwargs={}, raw=None, delta=None)}, {'test_name': 'evaluate_synthetic_judge', 'config': {'approach': 'sql_engine_w_reranking', 'database': {'path': 'sqlite:///data/chinook.db', 'metadata': {'metadata_path': 'sqlite:///data/chinook.db', 'table_name': 'table_descriptions', 'document_id_like': '%-2'}}, 'index': {'name': 'llama2reranker', 'class': 'ollama', 'retriever_kwargs': {'similarity_top_k': 4}, 'reranking': {'config_object': 'ReRankerLLMConfig', 'reranker_kwargs': {'top_n': 3}}}, 'llm': {'name': 'llama2', 'class': 'ollama'}}, 'input_query': 'Which was the most expensive item our top employee sold?', 'expected_response': 'The genre of music that has the longest song is \"TV Shows\".', 'actual_response': {'response': None, 'tables': ['invoice_items', 'playlist_track']}, 'synthesized_judgement': CompletionResponse(text=\"Sure! Based on the information provided in the reference text and the summary, I would give a score of 2 out of 10 for the summary. Here's why:\\n\\nThe summary provided is completely unrelated to the topic of the reference text. The reference text discusses the genre of music with the longest song, while the summary provides a completely different set of data and does not make any connection to the original topic. Therefore, I would score the summary as 2 out of 10.\\n\\nMajor differences between the reference and the summary include:\\n\\n* The reference text discusses the genre of music with the longest song, while the summary provides a completely different set of data (invoices and playlists).\\n* The reference text is focused on a specific topic within the music industry, while the summary is unrelated to any specific topic.\\n* The reference text uses a clear and concise structure, while the summary is disorganized and lacks coherence.\", additional_kwargs={}, raw=None, delta=None)}, {'test_name': 'evaluate_synthetic_judge', 'config': {'approach': 'sql_engine_w_reranking', 'database': {'path': 'sqlite:///data/chinook.db', 'metadata': {'metadata_path': 'sqlite:///data/chinook.db', 'table_name': 'table_descriptions', 'document_id_like': '%-2'}}, 'index': {'name': 'llama2reranker', 'class': 'ollama', 'retriever_kwargs': {'similarity_top_k': 4}, 'reranking': {'config_object': 'ReRankerLLMConfig', 'reranker_kwargs': {'top_n': 3}}}, 'llm': {'name': 'llama2', 'class': 'ollama'}}, 'input_query': 'Which genre of music has the longest song?', 'expected_response': 'Agent stopped due to iteration limit or time limit.', 'actual_response': {'response': None, 'tables': []}, 'synthesized_judgement': CompletionResponse(text='Based on the information provided, I would give a score of 2 out of 10 for the summary. The summary does not accurately convey the main idea of the reference text. Here are the major differences between the two:\\n\\n* The summary does not include any mention of an iteration limit or time limit, which are key elements in the reference text.\\n* The summary simply returns a dictionary with no values, while the reference text specifically states that the agent stopped due to one of these limits being reached.\\n* The summary does not provide any tables, while the reference text includes information about tables that are relevant to the context.\\n\\nTherefore, I would rate the summary as 2 out of 10 for its inaccuracies and lack of detail compared to the reference text.', additional_kwargs={}, raw=None, delta=None)}]\n"
     ]
    }
   ],
   "source": [
    "from dbchat.evaluation.utils import save_test_results\n",
    "from dbchat.evaluation.evaluate import evaluate_table_name_retrieval, evaluate_synthetic_judge\n",
    "\n",
    "results = evaluate_synthetic_judge( test_data_path = ROOT_DIR.parent.parent / \"examples/evaluation/queries.csv\",\n",
    "                                    config_path = config_path )\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_test_results( results, test_results_path = ROOT_DIR.parent / \"test_results\" / \"results.json\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance on retrieving the correct tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_table_name_retrieval( test_data_path = ROOT_DIR.parent.parent / \"examples/evaluation/queries.csv\",\n",
    "                                    config_path = config_path )\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_test_results( results, test_results_path = ROOT_DIR.parent / \"test_results\" / \"results.json\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
