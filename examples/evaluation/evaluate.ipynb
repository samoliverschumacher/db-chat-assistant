{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '1',\n",
      "  'note': 'chooses the correct table.',\n",
      "  'response': 'We have made a total of $75.24 in Berlin.',\n",
      "  'tables': 'invoices',\n",
      "  'user_query': 'How much money have we made in Berlin?'},\n",
      " {'id': '2',\n",
      "  'note': 'deals with NULL, verifies with user that BillingState is not unique '\n",
      "          'to the entire table',\n",
      "  'response': 'The state that made the most money is California (CA).',\n",
      "  'tables': 'invoices',\n",
      "  'user_query': 'Which state made the most money?'}]\n",
      "[{'id': '3',\n",
      "  'note': 'semantic meaning of \"Type of song\" is used to select genre table',\n",
      "  'response': 'The type of music that has the longest song is \"Occupation / '\n",
      "              'Precipice\".',\n",
      "  'tables': 'tracks,genres',\n",
      "  'user_query': 'Which type of music has the longest song?'},\n",
      " {'id': '4',\n",
      "  'note': 'longer chain of related entities',\n",
      "  'response': 'The genre of music that has the longest song is \"TV Shows\".',\n",
      "  'tables': 'employees,customers,invoices,invoice_items',\n",
      "  'user_query': 'Which was the most expensive item our top employee sold?'}]\n",
      "[{'id': '5',\n",
      "  'note': 'easier test than test=3',\n",
      "  'response': 'Agent stopped due to iteration limit or time limit.',\n",
      "  'tables': 'tracks,genres',\n",
      "  'user_query': 'Which genre of music has the longest song?'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from dbchat.evaluation.utils import load_evaluation_csv_data\n",
    "from dbchat import ROOT_DIR\n",
    "fpath = ROOT_DIR.parent.parent / \"examples/evaluation/queries.csv\"\n",
    "\n",
    "eval_data = load_evaluation_csv_data( fpath, stream=True, chunksize = 2)\n",
    "for d in eval_data:\n",
    "    pprint.pprint(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approach: sql_engine_w_reranking\n",
      "database:\n",
      "  metadata:\n",
      "    document_id_like: '%-2'\n",
      "    metadata_path: sqlite:///data/chinook.db\n",
      "    table_name: table_descriptions\n",
      "  path: sqlite:///data/chinook.db\n",
      "index:\n",
      "  class: ollama\n",
      "  name: llama2reranker\n",
      "  reranking:\n",
      "    config_object: ReRankerLLMConfig\n",
      "    reranker_kwargs:\n",
      "      top_n: 3\n",
      "  retriever_kwargs:\n",
      "    similarity_top_k: 4\n",
      "llm:\n",
      "  class: ollama\n",
      "  name: llama2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "config_path = ROOT_DIR.parent / \"tests/data/inputs/cfg_3.yml\"\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "print(yaml.dump(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite:////mnt/c/Users/ssch7/repos/db-chat-assistant/data/chinook.db\n",
      "sqlite:////mnt/c/Users/ssch7/repos/db-chat-assistant/data/chinook.db\n",
      "Debugging Query: SELECT TABLE_NAME, DESCRIPTION FROM table_descriptions WHERE TABLE_NAME IN ('albums','artists','customers','employees','genres','invoice_items','invoices','media_types','playlist_track','playlists','artists'0,'artists'1) AND DOCUMENT_ID LIKE '%-2'\n",
      "input_query='How much money have we made in Berlin?'\n",
      "response.response='Sorry, but I\\'m a large language model, I cannot provide a response to the query \"How much money have we made in Berlin?\" as the provided SQL statement is invalid. The error message indicates that there is a problem with the syntax of the SQL statement.\\n\\nTo generate a response to this query, I would need a valid and complete SQL statement that can be executed to retrieve the desired data from the database. Can you please provide me with the correct SQL statement or more information about the query so I can help you?'\n",
      "SELECT SUM(invoice_items.UnitPrice * invoice_items.Quantity) AS total_revenue FROM invoice_items JOIN playlist_track ON invoice_items.TrackId = playlist_track.TrackId JOIN tracks ON playlist_track.PlaylistId = tracks.PlaylistId WHERE tracks.City = 'Berlin';\n",
      "retrieved_tables=[SQLTableSchema(table_name='tracks', context_str='Tracks on a album, and details like price. Does not store a reference to playlist.'), SQLTableSchema(table_name='invoice_items', context_str='A record of the items on invoices, including the track those items occured on the album.'), SQLTableSchema(table_name='playlist_track', context_str='Maps track ids to playlist ids.')]\n"
     ]
    }
   ],
   "source": [
    "from dbchat.sql_agent import create_agent\n",
    "\n",
    "query_engine = create_agent( config )\n",
    "\n",
    "eval_data = load_evaluation_csv_data( fpath, stream=False)\n",
    "input_query = eval_data[0]['user_query']\n",
    "response = query_engine.query(input_query)\n",
    "print(f\"{input_query=}\"\n",
    "        \"\\n\"\n",
    "        f\"{response.response=}\"\n",
    "        \"\\n\"\n",
    "        f\"{response.metadata['sql_query']}\")\n",
    "retrieved_tables = query_engine.sql_retriever._get_tables(input_query)\n",
    "print(f\"{retrieved_tables=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging Query: SELECT TABLE_NAME, DESCRIPTION FROM table_descriptions WHERE TABLE_NAME IN ('albums','artists','customers','employees','genres','invoice_items','invoices','media_types','playlist_track','playlists','artists'0,'artists'1) AND DOCUMENT_ID LIKE '%-2'\n",
      "[{'test_name': 'evaluate_synthetic_judge', 'config': {'approach': 'sql_engine_w_reranking', 'database': {'path': 'sqlite:///data/chinook.db', 'metadata': {'metadata_path': 'sqlite:///data/chinook.db', 'table_name': 'table_descriptions', 'document_id_like': '%-2'}}, 'index': {'name': 'llama2reranker', 'class': 'ollama', 'retriever_kwargs': {'similarity_top_k': 4}, 'reranking': {'config_object': 'ReRankerLLMConfig', 'reranker_kwargs': {'top_n': 3}}}, 'llm': {'name': 'llama2', 'class': 'ollama'}}, 'input_query': 'How much money have we made in Berlin?', 'expected_response': 'We have made a total of $75.24 in Berlin.', 'actual_response': Response(response='Sorry, but I\\'m a large language model, I cannot provide a response to the query \"How much money have we made in Berlin?\" as the provided SQL statement is invalid. The error message indicates that there is a problem with the syntax of the SQL statement.\\n\\nTo generate a response to this query, I would need a valid and complete SQL statement that can be executed to retrieve the desired data from the database. Can you please provide me with the correct SQL statement or more information about the query so I can help you?', source_nodes=[NodeWithScore(node=TextNode(id_='54b114e1-86fa-4b3d-ac7f-29d0b681196a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='02819031f0a69f387d99798e7a70c72fd19c2ad4769df62701525c3d693e0ab4', text='Error: Statement \"SELECT SUM(invoice_items.UnitPrice * invoice_items.Quantity) AS total_revenue FROM invoice_items JOIN playlist_track ON invoice_items.TrackId = playlist_track.TrackId JOIN tracks ON playlist_track.PlaylistId = tracks.PlaylistId WHERE tracks.City = \\'Berlin\\';\" is invalid SQL.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)], metadata={'54b114e1-86fa-4b3d-ac7f-29d0b681196a': {}, 'sql_query': \"SELECT SUM(invoice_items.UnitPrice * invoice_items.Quantity) AS total_revenue FROM invoice_items JOIN playlist_track ON invoice_items.TrackId = playlist_track.TrackId JOIN tracks ON playlist_track.PlaylistId = tracks.PlaylistId WHERE tracks.City = 'Berlin';\"}), 'synthesized_judgement': CompletionResponse(text='Score: 2\\n\\nThe summary does not accurately reflect the content of the reference text. The reference text states a specific amount of money made in Berlin, while the summary provides an error message and asks for further clarification or information to generate a response.\\n\\nMajor differences between the reference and summary include:\\n\\n* The reference text provides a specific amount of money made in Berlin, while the summary does not provide any financial information.\\n* The reference text is a statement of fact, while the summary is an error message asking for further clarification.\\n* The reference text is written in a straightforward and objective tone, while the summary is more informal and includes a request for additional information.', additional_kwargs={}, raw=None, delta=None)}, {'test_name': 'evaluate_synthetic_judge', 'config': {'approach': 'sql_engine_w_reranking', 'database': {'path': 'sqlite:///data/chinook.db', 'metadata': {'metadata_path': 'sqlite:///data/chinook.db', 'table_name': 'table_descriptions', 'document_id_like': '%-2'}}, 'index': {'name': 'llama2reranker', 'class': 'ollama', 'retriever_kwargs': {'similarity_top_k': 4}, 'reranking': {'config_object': 'ReRankerLLMConfig', 'reranker_kwargs': {'top_n': 3}}}, 'llm': {'name': 'llama2', 'class': 'ollama'}}, 'input_query': 'Which state made the most money?', 'expected_response': 'The state that made the most money is California (CA).', 'actual_response': Response(response='Based on the query results provided, the state that made the most money is California. The total revenue for California is $765.27.', source_nodes=[NodeWithScore(node=TextNode(id_='6f9ce960-a000-4c3f-ad4f-060b0a6eff03', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='972c7e936f5a0b91f2a6bd13c5326cc31a3d3e4fc9c3c89dfefc129a991bbcd4', text='[(765.27,)]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)], metadata={'6f9ce960-a000-4c3f-ad4f-060b0a6eff03': {}, 'sql_query': 'SELECT SUM(Quantity * UnitPrice) AS total_revenue FROM invoice_items WHERE TrackId IN (SELECT TrackId FROM tracks WHERE MediaTypeId = 1 AND GenreId = 1);', 'result': [(765.27,)], 'col_keys': ['total_revenue']}), 'synthesized_judgement': CompletionResponse(text='Sure! Based on the information provided in the summary, I would give it a score of 8 out of 10. The summary provides a clear and concise statement of the main idea, which is that California made the most money among all states. The summary also includes a specific dollar amount, $765.27, to support the claim.\\n\\nHowever, there are a few minor differences between the reference text and the summary:\\n\\n* In the reference text, the word \"state\" is capitalized, while in the summary it is not.\\n* The reference text includes the full name of the state, \"California,\" while the summary uses the abbreviation \"CA.\"\\n* The reference text includes a figure of $765.27, which is the total revenue for California, while the summary simply states that California made the most money without providing the exact amount.\\n\\nOverall, the summary provides a good summary of the main idea and supporting details from the reference text, but there are some minor discrepancies in language and format.', additional_kwargs={}, raw=None, delta=None)}, {'test_name': 'evaluate_synthetic_judge', 'config': {'approach': 'sql_engine_w_reranking', 'database': {'path': 'sqlite:///data/chinook.db', 'metadata': {'metadata_path': 'sqlite:///data/chinook.db', 'table_name': 'table_descriptions', 'document_id_like': '%-2'}}, 'index': {'name': 'llama2reranker', 'class': 'ollama', 'retriever_kwargs': {'similarity_top_k': 4}, 'reranking': {'config_object': 'ReRankerLLMConfig', 'reranker_kwargs': {'top_n': 3}}}, 'llm': {'name': 'llama2', 'class': 'ollama'}}, 'input_query': 'Which type of music has the longest song?', 'expected_response': 'The type of music that has the longest song is \"Occupation / Precipice\".', 'actual_response': Response(response='Given the input question \"Which type of music has the longest song?\", and the query results from the SQL statement \"SELECT length(song) FROM Music WHERE genre = \\'Classical\\';\", we can synthesize a response as follows:\\n\\nThe longest song in classical music is \"Le Sacre du Printemps\" (The Rite of Spring) by Igor Stravinsky, which has a duration of approximately 20 minutes.\\n\\nThis information can be found through research and analysis of the query results, which provide the length of each song in the Music table where the genre is \\'Classical\\'. By sorting the results by length and selecting the longest song, we can determine the answer to the input question.', source_nodes=[NodeWithScore(node=TextNode(id_='1e0df425-12bf-4ecb-8d3e-8358dd46f173', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='4eccf53488e3395d87e784b5baf5acb28368be0ac30cdf2c88d2cc9d607cd5e5', text='Error: Statement \"SELECT length(song) FROM Music WHERE genre = \\'Classical\\';\" is invalid SQL.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)], metadata={'1e0df425-12bf-4ecb-8d3e-8358dd46f173': {}, 'sql_query': \"SELECT length(song) FROM Music WHERE genre = 'Classical';\"}), 'synthesized_judgement': CompletionResponse(text='Based on the information provided, I would rate the summary a 7 out of 10. The summary provides a correct answer to the input question and uses the information from the query results to support its response. However, the summary could be more detailed and provide additional context about the longest song in classical music, such as the composer, title, and year of composition.\\n\\nMajor differences between the reference and the summary are:\\n\\n* The reference states that the longest song is \"Occupation / Precipice\", while the summary states that it is \"Le Sacre du Printemps\" by Igor Stravinsky.\\n* The reference does not provide any context or explanation for how the length of the song was determined, while the summary includes information about the query results and how they were analyzed to determine the answer.\\n* The reference does not use a specific format for the response, while the summary uses a clear and concise structure to present the information.', additional_kwargs={}, raw=None, delta=None)}, {'test_name': 'evaluate_synthetic_judge', 'config': {'approach': 'sql_engine_w_reranking', 'database': {'path': 'sqlite:///data/chinook.db', 'metadata': {'metadata_path': 'sqlite:///data/chinook.db', 'table_name': 'table_descriptions', 'document_id_like': '%-2'}}, 'index': {'name': 'llama2reranker', 'class': 'ollama', 'retriever_kwargs': {'similarity_top_k': 4}, 'reranking': {'config_object': 'ReRankerLLMConfig', 'reranker_kwargs': {'top_n': 3}}}, 'llm': {'name': 'llama2', 'class': 'ollama'}}, 'input_query': 'Which was the most expensive item our top employee sold?', 'expected_response': 'The genre of music that has the longest song is \"TV Shows\".', 'actual_response': Response(response='The query you provided is invalid because it is missing a table alias or column name in the SELECT clause. The error message indicates that the SELECT statement should be rewritten to include an alias or column name for the table being selected from.\\n\\nHere is an example of how you could modify the query to make it valid:\\n```\\nSELECT InvoiceItems.UnitPrice * InvoiceItems.Quantity AS TotalCost\\nFROM invoice_items JOIN playlist_track ON InvoiceItems.TrackId = playlist_track.TrackId\\nWHERE InvoiceItems.InvoiceId = (SELECT InvoiceId FROM invoices WHERE EmployeeId = 1);\\n```\\nBy adding an alias (`TotalCost`) to the SELECT clause and specifying the table being selected from (`invoice_items`), the query becomes valid SQL.', source_nodes=[NodeWithScore(node=TextNode(id_='8c94fe0c-38b0-42e9-b61a-d7eb5ec0ba5e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='84cee24ca8eadcb9b9a20294f8b4cda4c252beee02337cfc6799b7eb3044543f', text=\"Error: Statement 'SELECT InvoiceItems.UnitPrice * InvoiceItems.Quantity AS TotalCost FROM invoice_items JOIN playlist_track ON InvoiceItems.TrackId = playlist_track.TrackId WHERE InvoiceItems.InvoiceId = (SELECT InvoiceId FROM invoices WHERE EmployeeId = 1);' is invalid SQL.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)], metadata={'8c94fe0c-38b0-42e9-b61a-d7eb5ec0ba5e': {}, 'sql_query': 'SELECT InvoiceItems.UnitPrice * InvoiceItems.Quantity AS TotalCost FROM invoice_items JOIN playlist_track ON InvoiceItems.TrackId = playlist_track.TrackId WHERE InvoiceItems.InvoiceId = (SELECT InvoiceId FROM invoices WHERE EmployeeId = 1);'}), 'synthesized_judgement': CompletionResponse(text='Thank you for providing the reference text and the summary to compare against. Based on the information provided, I would give the summary a score of 2 out of 10 for its accuracy. Here\\'s why:\\n\\n* The summary completely misinterprets the reference text by assuming that the query is invalid due to missing table aliases or column names, when in fact, the reference text does not contain any such information.\\n* The summary provides a incorrect solution by suggesting the addition of an alias and specifying the table being selected from, which is not necessary for the query provided in the reference text.\\n\\nMajor differences between the reference and the summary are:\\n\\n1. The reference text does not mention anything about a \"TV Shows\" genre of music. Instead, it talks about a query that is invalid due to missing table aliases or column names.\\n2. The summary provides a solution that is not applicable to the query provided in the reference text.\\n3. The summary assumes that the query is invalid without providing any evidence or context to support this claim.', additional_kwargs={}, raw=None, delta=None)}, {'test_name': 'evaluate_synthetic_judge', 'config': {'approach': 'sql_engine_w_reranking', 'database': {'path': 'sqlite:///data/chinook.db', 'metadata': {'metadata_path': 'sqlite:///data/chinook.db', 'table_name': 'table_descriptions', 'document_id_like': '%-2'}}, 'index': {'name': 'llama2reranker', 'class': 'ollama', 'retriever_kwargs': {'similarity_top_k': 4}, 'reranking': {'config_object': 'ReRankerLLMConfig', 'reranker_kwargs': {'top_n': 3}}}, 'llm': {'name': 'llama2', 'class': 'ollama'}}, 'input_query': 'Which genre of music has the longest song?', 'expected_response': 'Agent stopped due to iteration limit or time limit.', 'actual_response': Response(response='The query you provided is not a valid SQL query. The `SELECT` statement should be followed by a `FROM` clause that specifies the table or tables to retrieve data from, and the `WHERE` clause should be used to filter the results based on certain conditions. In this case, the `WHERE` clause is missing.\\n\\nTo answer your original question, the genre of music with the longest song would depend on how one defines \"longest.\" There are many classical pieces that are longer than 10 minutes, but the longest known classical piece is \"Gnossienne No. 1\" by Erik Satie, which lasts around 37 minutes. However, there are also other genres of music, such as ambient and drone, that have longer songs.\\n\\nHere\\'s an example response you could give:\\n\\n\"The genre of music with the longest song is difficult to determine as it depends on how one defines \\'longest.\\' However, according to some sources, the longest known classical piece, \\'Gnossienne No. 1\\' by Erik Satie, lasts around 37 minutes. However, other genres such as ambient and drone have longer songs. Can you provide more context or clarify your question?\"', source_nodes=[NodeWithScore(node=TextNode(id_='ed17a974-7a6f-4cf5-8051-400a42547698', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='d12c651cd9ed88c222e73d52afeb6500d76b2499daf8f7243320de9dc2aa95bb', text='Error: Statement \"SELECT length(song) FROM songs WHERE genre = \\'Classical\\';\" is invalid SQL.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)], metadata={'ed17a974-7a6f-4cf5-8051-400a42547698': {}, 'sql_query': \"SELECT length(song) FROM songs WHERE genre = 'Classical';\"}), 'synthesized_judgement': CompletionResponse(text='Thank you for providing the reference text and the summary to compare against. Based on the provided information, I would give a score of 6 out of 10 for the summary. The summary provides some basic information about the length of songs in different genres of music, but it does not accurately summarize the original text. Here are some major differences between the reference and the summary:\\n\\n* The summary does not mention the specific genre of music mentioned in the reference text (classical).\\n* The summary provides a vague answer to the question asked in the reference text, while the reference text provides a more specific and detailed answer.\\n* The summary does not address the issue raised in the reference text about the lack of a `WHERE` clause in the provided SQL query.\\n\\nTo improve the summary, it would be necessary to provide more accurate information about the genre of music with the longest song, as well as address the issue raised in the reference text. Here is an example response that could be given based on the provided information:\\n\\n\"The genre of music with the longest song is difficult to determine as it depends on how one defines \\'longest.\\' However, according to some sources, the longest known classical piece, \\'Gnossienne No. 1\\' by Erik Satie, lasts around 37 minutes. Can you provide more context or clarify your question?\"\\n\\nBullet point list of major differences between the reference and the summary:\\n\\n* The genre of music mentioned in the reference text is not mentioned in the summary.\\n* The summary provides a vague answer to the question asked in the reference text, while the reference text provides a more specific and detailed answer.\\n* The summary does not address the issue raised in the reference text about the lack of a `WHERE` clause in the provided SQL query.', additional_kwargs={}, raw=None, delta=None)}]\n"
     ]
    }
   ],
   "source": [
    "from dbchat.evaluation.utils import save_test_results\n",
    "from dbchat.evaluation.evaluate import evaluate_table_name_retrieval, evaluate_synthetic_judge\n",
    "\n",
    "results = evaluate_synthetic_judge( test_data_path = ROOT_DIR.parent.parent / \"examples/evaluation/queries.csv\",\n",
    "                                    config_path = config_path )\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 684 (char 683)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/ssch7/repos/db-chat-assistant/examples/evaluation/evaluate.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ssch7/repos/db-chat-assistant/examples/evaluation/evaluate.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m save_test_results( results, test_results_path \u001b[39m=\u001b[39m ROOT_DIR\u001b[39m.\u001b[39mparent \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_results\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mresults.json\u001b[39m\u001b[39m\"\u001b[39m )\n",
      "File \u001b[0;32m/mnt/c/Users/ssch7/repos/db-chat-assistant/src/dbchat/evaluation/utils.py:18\u001b[0m, in \u001b[0;36msave_test_results\u001b[0;34m(results, test_results_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m ( test_results_path )\u001b[39m.\u001b[39mexists():\n\u001b[1;32m     17\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m( test_results_path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m ) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m---> 18\u001b[0m         data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload( file )\n\u001b[1;32m     19\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     data \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/db-chat/lib/python3.11/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, object_hook\u001b[39m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39mparse_float, parse_int\u001b[39m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39mparse_constant, object_pairs_hook\u001b[39m=\u001b[39mobject_pairs_hook, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/db-chat/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/envs/db-chat/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_decode(s, idx\u001b[39m=\u001b[39m_w(s, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/anaconda3/envs/db-chat/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 684 (char 683)"
     ]
    }
   ],
   "source": [
    "save_test_results( results, test_results_path = ROOT_DIR.parent / \"test_results\" / \"results.json\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance on retrieving the correct tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite:////mnt/c/Users/ssch7/repos/db-chat-assistant/data/chinook.db\n",
      "sqlite:////mnt/c/Users/ssch7/repos/db-chat-assistant/data/chinook.db\n",
      "Debugging Query: SELECT TABLE_NAME, DESCRIPTION FROM table_descriptions WHERE TABLE_NAME IN ('albums','artists','customers','employees','genres','invoice_items','invoices','media_types','playlist_track','playlists','artists'0,'artists'1) AND DOCUMENT_ID LIKE '%-2'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/ssch7/repos/db-chat-assistant/examples/evaluation/evaluate.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ssch7/repos/db-chat-assistant/examples/evaluation/evaluate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m evaluate_table_name_retrieval( test_data_path \u001b[39m=\u001b[39m ROOT_DIR\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mparent \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mexamples/evaluation/queries.csv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ssch7/repos/db-chat-assistant/examples/evaluation/evaluate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                                     config_path \u001b[39m=\u001b[39m config_path )\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ssch7/repos/db-chat-assistant/examples/evaluation/evaluate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(results)\n",
      "File \u001b[0;32m/mnt/c/Users/ssch7/repos/db-chat-assistant/src/dbchat/evaluation/evaluate.py:26\u001b[0m, in \u001b[0;36mevaluate_table_name_retrieval\u001b[0;34m(test_data_path, config_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m data:\n\u001b[0;32m---> 26\u001b[0m     input_query, expected_tables \u001b[39m=\u001b[39m row[ \u001b[39m'\u001b[39m\u001b[39muser_query\u001b[39m\u001b[39m'\u001b[39m ], row[ \u001b[39m'\u001b[39m\u001b[39mtables\u001b[39m\u001b[39m'\u001b[39m ]\u001b[39m.\u001b[39msplit( \u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m )\n\u001b[1;32m     28\u001b[0m     \u001b[39m# Retrieve\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[39mif\u001b[39;00m cfg[ \u001b[39m'\u001b[39m\u001b[39mapproach\u001b[39m\u001b[39m'\u001b[39m ] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msql_engine_w_reranking\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "results = evaluate_table_name_retrieval( test_data_path = ROOT_DIR.parent.parent / \"examples/evaluation/queries.csv\",\n",
    "                                    config_path = config_path )\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_test_results( results, test_results_path = ROOT_DIR.parent / \"test_results\" / \"results.json\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
